{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initalization"
      ],
      "metadata": {
        "id": "t0mr45SErwLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GENDWz2o8Az"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in file created with  SINGLEFILE  extension\n",
        "with open('/content/(20_) Hollis Queens NY _ Facebook (11_13_2024 2_47_24 PM).html', 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "crRAXbFdp2Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parse through soup and separate all post data\n",
        "captionsWithSep=[]\n",
        "outer_divs = soup.find_all('div', class_='x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z')\n",
        "for outer_div in outer_divs:\n",
        "    if outer_div:\n",
        "      captionsWithSep.append(outer_div.get_text(strip=True,separator=\"$$\"))"
      ],
      "metadata": {
        "id": "UalLvdWoqKGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction"
      ],
      "metadata": {
        "id": "dKCRMDdmrLqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Poster Names"
      ],
      "metadata": {
        "id": "ytgcWXLFrPoG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW7pJr-uXPSF"
      },
      "outputs": [],
      "source": [
        "def extactPosterNames(data_list):\n",
        "    \"\"\"\n",
        "    Extracts the name of poster held within the 33rd separation '$$' as the separator\n",
        "    from each list member\n",
        "\n",
        "    Args:\n",
        "        data_list: A list of strings, where each string contains each post's data separated by '$$'.\n",
        "\n",
        "    Returns:\n",
        "        A list where each member is the name of the poster.\n",
        "    \"\"\"\n",
        "\n",
        "    extracted_words = []\n",
        "    for data_str in data_list:\n",
        "        parts = data_str.split(\"$$\")\n",
        "        if len(parts) >= 33:  # Check if the 33rd separation exists\n",
        "            extracted_word = parts[33]  # Name is always held in 33rd position\n",
        "            extracted_words.append(extracted_word)\n",
        "        else:\n",
        "            extracted_words.append(None)\n",
        "\n",
        "    return extracted_words\n",
        "\n",
        "extracted_words = extactPosterNames(captionsWithSep)\n",
        "#Manual edit for post not pertaining to current project\n",
        "del extracted_words[25]\n",
        "print(extracted_words)\n",
        "print(len(extracted_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract post text"
      ],
      "metadata": {
        "id": "pZlgOLMnrS2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_post(entries):\n",
        "  \"\"\"\n",
        "  Extracts data from a list of entries, where each entry is a string\n",
        "  containing data separated by \"$$\".\n",
        "\n",
        "  For each entry, it finds the data within a separator which says \"Shared with Private group\" and saves the data at the following separator.\n",
        "  If there is aditional data held after that separtor, it saves that data as well.\n",
        "\n",
        "  Args:\n",
        "    entries: A list of strings, where each string is all that post's data.\n",
        "\n",
        "  Returns:\n",
        "    A list of strings, where each string contains the extracted data.\n",
        "  \"\"\"\n",
        "\n",
        "  extracted_data = []\n",
        "  for entry in entries:\n",
        "    parts = entry.split(\"$$\")\n",
        "    for i, part in enumerate(parts):\n",
        "      if \"Shared with Private group\" in part:\n",
        "        try:\n",
        "\n",
        "          if len(parts[i+1])<3:\n",
        "            extracted_data.append(\"NULL\")\n",
        "          elif len(parts[i+3])>3:\n",
        "            dataToAppend=parts[i+1]+\" \"+parts[i+2]+\" \"+parts[i+3]\n",
        "            extracted_data.append(dataToAppend)\n",
        "          elif len(parts[i+2])>3:\n",
        "            dataToAppend=parts[i+1]+\" \"+parts[i+2]\n",
        "            extracted_data.append(dataToAppend)\n",
        "          else:\n",
        "            dataToAppend=parts[i+1]\n",
        "            extracted_data.append(dataToAppend)\n",
        "\n",
        "          break  #Move to the next entry\n",
        "        except IndexError:\n",
        "          # if its at the end, append blank\n",
        "          extracted_data.append(\"\")\n",
        "          break\n",
        "    else:\n",
        "      # If \"Shared with Private group\" is not found in the entry\n",
        "      extracted_data.append(\"\")\n",
        "\n",
        "  return extracted_data\n",
        "\n",
        "\n",
        "extracted_results = extract_post(captionsWithSep)\n",
        "\n",
        "\n",
        "print(extracted_results)\n",
        "#Manual edit for post not pertaining to current project\n",
        "del extracted_results[25]\n",
        "print(len(extracted_results))"
      ],
      "metadata": {
        "id": "taU9_pSurU8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract image data"
      ],
      "metadata": {
        "id": "zW74WZoVrcfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bse64Img=[]\n",
        "img_tags = soup.find_all('img', class_=re.compile('x1ey2m1c xds687c x5yr21d x10l6tqk x17qophe x13vifvy xh8yej3'))\n",
        "\n",
        "i = 0  # Initialize the index variable\n",
        "while i < len(img_tags):\n",
        "    img_tag = img_tags[i]\n",
        "    img_url = img_tag.get('src')\n",
        "    alt_text = img_tag.get('alt', '')\n",
        "\n",
        "    if alt_text:\n",
        "        bse64Img.append(img_url)\n",
        "        i += 1  # Move to the next image\n",
        "    else:\n",
        "        # Check if the next image exists to avoid IndexError\n",
        "        if i + 1 < len(img_tags):\n",
        "            next_img_url = img_tags[i + 1].get('src')\n",
        "            bse64Img.append([img_url, next_img_url])\n",
        "            i += 2  # Skip the next image (move ahead by 2)\n",
        "        else:\n",
        "            # If no next image exists, just move to the next image\n",
        "            i += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wH7A0RW4refI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining all data into Json"
      ],
      "metadata": {
        "id": "qj8lXAEjrl51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bse64Img=[]\n",
        "img_tags = soup.find_all('img', class_=re.compile('x1ey2m1c xds687c x5yr21d x10l6tqk x17qophe x13vifvy xh8yej3'))\n",
        "\n",
        "i = 0  # Initialize the index variable\n",
        "while i < len(img_tags):\n",
        "    img_tag = img_tags[i]\n",
        "    img_url = img_tag.get('src')\n",
        "    alt_text = img_tag.get('alt', '')\n",
        "\n",
        "    if alt_text:\n",
        "        bse64Img.append(img_url)\n",
        "        i += 1\n",
        "    else:\n",
        "        # Check if the next image exists to avoid IndexError\n",
        "        if i + 1 < len(img_tags):\n",
        "            next_img_url = img_tags[i + 1].get('src')\n",
        "            bse64Img.append([img_url, next_img_url])\n",
        "            i += 2  # Skip the next image (move ahead by 2)\n",
        "        else:\n",
        "            # No image, move on\n",
        "            i += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OuuhVus9rtUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the JSON file\n",
        "with open(\"output.json\", \"r\") as json_file:\n",
        "    allDataDict = json.load(json_file)\n",
        "\n",
        "for entry_key, entry_data in allDataDict.items():\n",
        "    print(f\"Entry {entry_key}:\")\n",
        "    print(f\"  Group Name: {entry_data['groupName']}\")\n",
        "    print(f\"  Poster: {entry_data['poster']}\")\n",
        "    print(f\"  Text: {entry_data['text']}\")\n",
        "\n",
        "    # Decode the base64 image data\n",
        "    picData = entry_data['picData']\n",
        "    if isinstance(picData, list):\n",
        "        for idx, pic in enumerate(picData):\n",
        "          display(Image(url=pic))\n",
        "    else:\n",
        "      display(Image(url=picData))\n"
      ],
      "metadata": {
        "id": "Pm8GkgYSwhuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}